## THIS FILE HOLDS ALL MODELS_ML_FINAL HYPERPARAMETERS

# LightGBM: static parameters
lgbm_static_params:
    boosting_type: 'dart'
    extra_trees: True
    n_jobs: 23
    n_iter: 100
    scoring: 'neg_root_mean_squared_error'
    early_stopping_round: 50

# LightGBM features
lgbm_features:
      - sector
      - market_cap_cat
      - roll_30day_med_vol
      - mom_1_0
      - mom_3_0
      - mom_6_0
      - mom_12_0
      - mom_1_0_L1
      - mom_1_0_L2
      - mom_1_0_L3
      - mom_1_0_L4
      - mom_1_0_L5
      - mom_1_0_L6
      - mom_1_0_L7
      - mom_1_0_L8
      - mom_1_0_L9
      - mom_1_0_L10
      - mom_1_0_L11
      - mom_1_0_L12
      - mom_1_0_L13
      - mom_1_0_L14
      - mom_1_0_L15
      - mom_1_0_L16
      - mom_1_0_L17
      - mom_1_0_L18
      - mom_1_0_L19
      - mom_1_0_L20
      - mom_1_0_L21
      - mom_1_0_L22
      - mom_1_0_L23
      - mom_1_0_L24


## LIGHTGBM COARSE TUNING
# LightGBM: coarse tuning parameters
lgbm_coarse_tune_params:
    lgbm_model__n_estimators: [ 50, 100, 150, 200, 250, 300, 350, 400, 425, 475, 545, 616, 687, 758, 829,
                                900, 970, 1041, 1112, 1183, 1254, 1325, 1395, 1466, 1537, 1608, 1679, 1750 ]
    lgbm_model__learning_rate: [ 0.01 , 0.012, 0.013, 0.015, 0.017, 0.018, 0.02 , 0.022, 0.023,
                                 0.025, 0.027, 0.028, 0.03 , 0.032, 0.033, 0.035, 0.037, 0.038,
                                 0.04 , 0.042, 0.043, 0.045, 0.047, 0.048, 0.05 ]
    lgbm_model__num_leaves: [ 35,  43,  51,  59,  68,  76,  84,  92, 101,
                              109, 117, 125, 134, 142, 150, 158, 167, 175, 183, 191, 200 ]
    lgbm_model__min_data_in_leaf: [ 20, 40, 60, 80, 100, 125, 150, 175, 200, 222, 245, 268, 291, 314, 337, 360 ]
    lgbm_model__reg_lambda: [ 2.0, 2.25, 2.5, 3.75, 3.0, 3.25, 3.5, 3.75, 4.0, 4.25, 4.5, 4.75,
                              5.0, 5.625, 6.25, 6.875, 7.5, 8.125, 8.75, 9.375,
                              10.0, 10.625, 11.25, 11.875, 12.5, 13.125, 13.75, 14.375,
                              15.0, 15.625, 16.25, 16.875, 17.5, 18.125, 18.75 , 19.375,
                              20.0 ]


# LightGBM best coarse tuning parameters (not used in any pipeline)
lgbm_coarse_best_params:
    n_estimators: 262
    learning_rate: 0.012
    max_depth: 222
    reg_lambda: 6.25
    num_leaves: 2
    max_bin: 296



## LIGHTGBM FINE-TUNING
# LightGBM: fine-tuning parameters
lgbm_fine_tune_params:
    lgbm_model__n_estimators: [ 50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170,
                               180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                               310, 320, 330, 340, 350 ]
    lgbm_model__learning_rate: [ 0.01, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019, 0.02 ]
    lgbm_model__num_leaves: [ 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,
                              36, 38, 40, 42, 44, 46, 48, 50 ]
    lgbm_model__min_data_in_leaf: [ 200, 222, 245, 268, 291, 314, 337, 360, 383, 406, 429, 452, 475,
                                    497, 520, 543, 566, 589, 612, 635, 658, 681, 704, 727, 750 ]
    lgbm_model__reg_lambda: [ 5.0, 5.625, 6.25, 6.875, 7.5, 8.125, 8.75, 9.375,
                             10.0, 10.625, 11.25, 11.875, 12.5, 13.125, 13.75, 14.375,
                             15.0, 15.625, 16.25, 16.875, 17.5, 18.125, 18.75 , 19.375, 20.0 ]



### Model hyperparameters ###
# LightGBM best fine-tuning parameters
lgbm_fine_best_params:
    n_estimators: 140
    learning_rate: 0.012
    max_depth: 350
    reg_lambda: 10.625
    num_leaves: 6
    max_bin: 460
    random_state: 126502
### Model hyperparameters ###



################################ THIS HOLDS ALL MODELS_ML_EXPLORATORY HYPERPARAMETERS ##################################

# Random Forest: static parameters
rfr_static_params:
    n_jobs: 23
    n_iter: 50

# Random Forest tuning parameters
rfr_tune_params:
    rfr_model__n_estimators: [50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170,
                              180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                              310, 320, 330, 340, 350]
    rfr_model__max_depth: [100, 110, 120, 130, 140, 150, 160, 170,
                           180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                           310, 320, 330, 340, 350]
    rfr_model__min_samples_leaf: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]
    rfr_model__min_samples_split: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]


# XGBoost: static parameters
xgb_static_params:
    n_jobs: 23
    n_iter: 50
    booster: 'gbtree'

# Random Forest tuning parameters
xgb_tune_params:
    xgb_model__n_estimators: [50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170,
                              180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                              310, 320, 330, 340, 350]
    xgb_model__eta: [0.01, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019, 0.02]
    xgb_model__lambda: [5.0, 5.625, 6.25, 6.875, 7.5, 8.125, 8.75, 9.375,
                        10.0, 10.625, 11.25, 11.875, 12.5, 13.125, 13.75, 14.375,
                        15.0, 15.625, 16.25, 16.875, 17.5, 18.125, 18.75 , 19.375, 20.0]
    xgb_model__gamma: [ 5.0, 5.625, 6.25, 6.875, 7.5, 8.125, 8.75, 9.375,
                        10.0, 10.625, 11.25, 11.875, 12.5, 13.125, 13.75, 14.375,
                        15.0, 15.625, 16.25, 16.875, 17.5, 18.125, 18.75 , 19.375, 20.0 ]
    xgb_model__max_depth: [10, 30, 50, 70, 85, 100, 110, 120, 130, 140, 150, 160, 170,
                           180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                           310, 320, 330, 340, 350]