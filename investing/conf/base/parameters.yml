############################################### PARAMETERS FILE ########################################################
###################################### FEATURE ENGINEERING PARAMETERS ##################################################
# List of variables to create change indices
change_index_list:
  - p_b
  - p_fcf_ttm

# List of variables to create rolling sums
sum_list:
  - return_index

# List of variables to create rolling averages
avg_list:
  - p_b_index
  - p_fcf_ttm_index

# List of variables to create 30-day rolling median
med30_list:
  - volume

# List of features for 'create_momentum_factors' method
momentum_features:
  - date
  - ticker
  - sector
  - market_cap
  - market_cap_cat
  - p_b
  - p_fcf_ttm
  - roll_30day_med_volume
  - return_index
  - p_b_index
  - p_fcf_ttm_index
  - roll_7day_sum_return_index
  - roll_11day_sum_return_index
  - roll_7day_sum_return_index_1m_lag
  - roll_11day_sum_return_index_3m_lag
  - roll_11day_sum_return_index_6m_lag
  - roll_11day_sum_return_index_12m_lag
  - roll_7day_avg_p_b_index
  - roll_11day_avg_p_b_index
  - roll_7day_avg_p_b_index_1m_lag
  - roll_11day_avg_p_b_index_3m_lag
  - roll_11day_avg_p_b_index_6m_lag
  - roll_11day_avg_p_b_index_12m_lag
  - roll_7day_avg_p_fcf_ttm_index
  - roll_11day_avg_p_fcf_ttm_index
  - roll_7day_avg_p_fcf_ttm_index_1m_lag
  - roll_11day_avg_p_fcf_ttm_index_3m_lag
  - roll_11day_avg_p_fcf_ttm_index_6m_lag
  - roll_11day_avg_p_fcf_ttm_index_12m_lag

# Lists of rolling features which are the numerator in creating 1-, 3-, 6-, and 12-month momentum values.
# Each value in these lists must coincide with a value in the corresponding 'mom_denominators_list' below.
# List for one-month momentum numerator values (separate because these will have lagged values).
mom_1m_numerators_list:
  - roll_7day_sum_return_index
  - roll_7day_avg_p_b_index
  - roll_7day_avg_p_fcf_ttm_index

# List of 3-, 6-, and 12-month momentum numerator values.
mom_numerators_list:
  - roll_11day_sum_return_index
  - roll_11day_avg_p_b_index
  - roll_11day_avg_p_fcf_ttm_index

# Lists of rolling features which are the denominator in creating 1-, 3-, 6-, and 12-month momentum values.
# Each value in these lists must coincide with a value in the corresponding 'mom_numerators_list' above.
# List for one-month momentum denominator values (separate because these will have lagged values).
mom_1m_denominators_list:
  - roll_7day_sum_return_index_1m_lag
  - roll_7day_avg_p_b_index_1m_lag
  - roll_7day_avg_p_fcf_ttm_index_1m_lag

# List of 3-, 6-, and 12-month momentum denominator values.
mom_3m_denominators_list:
  - roll_11day_sum_return_index_3m_lag
  - roll_11day_avg_p_b_index_3m_lag
  - roll_11day_avg_p_fcf_ttm_index_3m_lag

mom_6m_denominators_list:
  - roll_11day_sum_return_index_6m_lag
  - roll_11day_avg_p_b_index_6m_lag
  - roll_11day_avg_p_fcf_ttm_index_6m_lag

mom_12m_denominators_list:
  - roll_11day_sum_return_index_12m_lag
  - roll_11day_avg_p_b_index_12m_lag
  - roll_11day_avg_p_fcf_ttm_index_12m_lag

# List of resulting feature names from momentum feature creation.
# Each value in this list must coincide with a value in the 'mom_numerators_list' and 'mom_denominators_list' above
mom_feature_names_list:
  - return
  - pb
  - fcf_ttm

# Top of range for 1-month lagged variables (i.e., 25 means lags 1 to 24).
lag_range: 25

# List of variables to drop from 'momentum_data' to create final modeling data
modeling_data_drop_list:
  - market_cap
  - return_index
  - p_b_index
  - p_fcf_ttm_index
  - roll_7day_sum_return_index
  - roll_7day_avg_p_b_index
  - roll_7day_avg_p_fcf_ttm_index
  - roll_7day_sum_return_index_1m_lag
  - roll_7day_avg_p_b_index_1m_lag
  - roll_7day_avg_p_fcf_ttm_index_1m_lag
  - roll_11day_sum_return_index
  - roll_11day_avg_p_b_index
  - roll_11day_avg_p_fcf_ttm_index
  - roll_11day_sum_return_index_3m_lag
  - roll_11day_avg_p_b_index_3m_lag
  - roll_11day_avg_p_fcf_ttm_index_3m_lag
  - roll_11day_sum_return_index_6m_lag
  - roll_11day_avg_p_b_index_6m_lag
  - roll_11day_avg_p_fcf_ttm_index_6m_lag
  - roll_11day_sum_return_index_12m_lag
  - roll_11day_avg_p_b_index_12m_lag
  - roll_11day_avg_p_fcf_ttm_index_12m_lag


################################## THIS HOLDS ALL MODELS_ML_FINAL HYPERPARAMETERS ######################################

# Current Date
# Last end-of-month date available in data set (e.g. if it is 11/1/21 then current_date is 10/29/21)
# This date must be a valid trading date for the NYSE
current_date: '2021-10-29'

# Date to split train/validation sets from the final holdout set.
# The date given is the date that starts the holdout set (i.e., the train/validation sets will be all dates less than
# the date given).
modeling_data_date: '2020-02-01'

# Modeling Target
model_target:
  - target_1m_mom_lead

# Modeling features
model_features:
  - sector
  - market_cap_cat
  - p_b
  - p_fcf_ttm
  - roll_30day_med_volume
  - return_mom_1_0
  - pb_mom_1_0
  - fcf_ttm_mom_1_0
  - return_mom_3_0
  - pb_mom_3_0
  - fcf_ttm_mom_3_0
  - return_mom_6_0
  - pb_mom_6_0
  - fcf_ttm_mom_6_0
  - return_mom_12_0
  - pb_mom_12_0
  - fcf_ttm_mom_12_0
  - return_mom_1_0_L1
  - return_mom_1_0_L2
  - return_mom_1_0_L3
  - return_mom_1_0_L4
  - return_mom_1_0_L5
  - return_mom_1_0_L6
  - return_mom_1_0_L7
  - return_mom_1_0_L8
  - return_mom_1_0_L9
  - return_mom_1_0_L10
  - return_mom_1_0_L11
  - return_mom_1_0_L12
  - return_mom_1_0_L13
  - return_mom_1_0_L14
  - return_mom_1_0_L15
  - return_mom_1_0_L16
  - return_mom_1_0_L17
  - return_mom_1_0_L18
  - return_mom_1_0_L19
  - return_mom_1_0_L20
  - return_mom_1_0_L21
  - return_mom_1_0_L22
  - return_mom_1_0_L23
  - return_mom_1_0_L24
  - pb_mom_1_0_L1
  - pb_mom_1_0_L2
  - pb_mom_1_0_L3
  - pb_mom_1_0_L4
  - pb_mom_1_0_L5
  - pb_mom_1_0_L6
  - pb_mom_1_0_L7
  - pb_mom_1_0_L8
  - pb_mom_1_0_L9
  - pb_mom_1_0_L10
  - pb_mom_1_0_L11
  - pb_mom_1_0_L12
  - pb_mom_1_0_L13
  - pb_mom_1_0_L14
  - pb_mom_1_0_L15
  - pb_mom_1_0_L16
  - pb_mom_1_0_L17
  - pb_mom_1_0_L18
  - pb_mom_1_0_L19
  - pb_mom_1_0_L20
  - pb_mom_1_0_L21
  - pb_mom_1_0_L22
  - pb_mom_1_0_L23
  - pb_mom_1_0_L24
  - fcf_ttm_mom_1_0_L1
  - fcf_ttm_mom_1_0_L2
  - fcf_ttm_mom_1_0_L3
  - fcf_ttm_mom_1_0_L4
  - fcf_ttm_mom_1_0_L5
  - fcf_ttm_mom_1_0_L6
  - fcf_ttm_mom_1_0_L7
  - fcf_ttm_mom_1_0_L8
  - fcf_ttm_mom_1_0_L9
  - fcf_ttm_mom_1_0_L10
  - fcf_ttm_mom_1_0_L11
  - fcf_ttm_mom_1_0_L12
  - fcf_ttm_mom_1_0_L13
  - fcf_ttm_mom_1_0_L14
  - fcf_ttm_mom_1_0_L15
  - fcf_ttm_mom_1_0_L16
  - fcf_ttm_mom_1_0_L17
  - fcf_ttm_mom_1_0_L18
  - fcf_ttm_mom_1_0_L19
  - fcf_ttm_mom_1_0_L20
  - fcf_ttm_mom_1_0_L21
  - fcf_ttm_mom_1_0_L22
  - fcf_ttm_mom_1_0_L23
  - fcf_ttm_mom_1_0_L24


## LightGBM: static parameters
lgbm_static_params:
    boosting_type: 'dart'
    extra_trees: False
    n_jobs: 23
    n_iter: 400
    scoring: 'neg_root_mean_squared_error'


## LIGHTGBM COARSE TUNING
# LightGBM: coarse tuning parameters
lgbm_coarse_tune_params:
    lgbm_model__n_estimators: [ 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360,
                                380, 400, 420, 440, 460, 480, 500]
    lgbm_model__learning_rate: [ 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.002, 0.005, 0.01,
                                 0.012, 0.013, 0.015, 0.017, 0.018, 0.02 ]
    lgbm_model__max_depth: [ 2, 3, 4, 5, 6, 7, 8, 9, 10 ]
    lgbm_model__num_leaves: [ 4, 8, 16, 32, 64, 128, 256, 512, 1024 ]
    lgbm_model__min_data_in_leaf: [ 20, 40, 60, 80, 100, 125, 150, 175, 200, 222, 245, 268, 291, 314, 337, 360,
                                    383, 406, 429, 452, 475, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500, 4000,
                                    4500, 5000 ]


## LIGHTGBM FINE-TUNING
# LightGBM: fine-tuning parameters
lgbm_fine_tune_params:
    lgbm_model__n_estimators: [ 50,  56,  63,  70,  77,  84,  91,  98, 105, 112, 118, 125, 132,
                                139, 146, 153, 160, 167, 174, 181, 187, 194, 201, 208, 215, 222,
                                229, 236, 243, 250 ]
    lgbm_model__learning_rate: [ 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.002 ]
    lgbm_model__max_depth: [ 2, 3, 4, 5, 6, 7, 8, 9, 10 ]
    lgbm_model__num_leaves: [ 4, 8, 16, 32, 64, 128, 256, 512, 1024 ]
    lgbm_model__min_data_in_leaf: [ 2000, 2103, 2206, 2310, 2413, 2517, 2620, 2724, 2827, 2931, 3034,
                                    3137, 3241, 3344, 3448, 3551, 3655, 3758, 3862, 3965, 4068, 4172,
                                    4275, 4379, 4482, 4586, 4689, 4793, 4896, 5000 ]


### Model Best Hyperparameters ###
## LightGBM best fine-tuning parameters (modified slightly from tuning) ##
lgbm_fine_best_params:
    n_estimators: 250
    learning_rate: 0.0006
    max_depth: 7
    num_leaves: 80
    min_data_in_leaf: 5000
    boosting_type: 'dart'
    n_jobs: 23
    random_state: 638552

###  End Model hyperparameters ###



###################################### MODELS_ML_HOLDOUT_EVAL PARAMETERS ###############################################
# Monthly dates to loop through for validation. Train on everything less than each date and predict on next date's data.
# Use 'LightGBM best fine-tuning parameters' parameter
# This replicates real-world use.
# Train dates
validation_train_dates_list: [ '2020-02-01', '2020-03-01', '2020-04-01', '2020-05-01', '2020-06-01', '2020-07-01',
                               '2020-08-01', '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01',
                               '2021-02-01', '2021-03-01', '2021-04-01', '2021-05-01', '2021-06-01', '2021-07-01',
                               '2021-08-01', '2021-09-01' ]

# Test dates
validation_test_dates_list: [ '2020-03-01', '2020-04-01', '2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01',
                              '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01', '2021-01-01', '2021-02-01',
                              '2021-03-01', '2021-04-01', '2021-05-01', '2021-06-01', '2021-07-01', '2021-08-01',
                              '2021-09-01', '2021-10-01' ]



















################################ THIS HOLDS ALL MODELS_ML_EXPLORATORY HYPERPARAMETERS ##################################

# Random Forest: static parameters
rfr_static_params:
    n_jobs: 23
    n_iter: 50

# Random Forest tuning parameters
rfr_tune_params:
    rfr_model__n_estimators: [50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170,
                              180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                              310, 320, 330, 340, 350]
    rfr_model__max_depth: [100, 110, 120, 130, 140, 150, 160, 170,
                           180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                           310, 320, 330, 340, 350]
    rfr_model__min_samples_leaf: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]
    rfr_model__min_samples_split: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]


# XGBoost: static parameters
xgb_static_params:
    n_jobs: 23
    n_iter: 50
    booster: 'gbtree'

# Random Forest tuning parameters
xgb_tune_params:
    xgb_model__n_estimators: [50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170,
                              180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                              310, 320, 330, 340, 350]
    xgb_model__eta: [0.01, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019, 0.02]
    xgb_model__lambda: [5.0, 5.625, 6.25, 6.875, 7.5, 8.125, 8.75, 9.375,
                        10.0, 10.625, 11.25, 11.875, 12.5, 13.125, 13.75, 14.375,
                        15.0, 15.625, 16.25, 16.875, 17.5, 18.125, 18.75 , 19.375, 20.0]
    xgb_model__gamma: [ 5.0, 5.625, 6.25, 6.875, 7.5, 8.125, 8.75, 9.375,
                        10.0, 10.625, 11.25, 11.875, 12.5, 13.125, 13.75, 14.375,
                        15.0, 15.625, 16.25, 16.875, 17.5, 18.125, 18.75 , 19.375, 20.0 ]
    xgb_model__max_depth: [10, 30, 50, 70, 85, 100, 110, 120, 130, 140, 150, 160, 170,
                           180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,
                           310, 320, 330, 340, 350]
